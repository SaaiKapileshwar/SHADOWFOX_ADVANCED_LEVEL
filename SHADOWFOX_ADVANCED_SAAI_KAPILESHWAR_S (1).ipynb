{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 1\n",
        "\n",
        "SENTIMENTAL ANALYSIS CODE WITHOUT HUGGING FACE."
      ],
      "metadata": {
        "id": "v3JLlrZWvx6D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb3dac7"
      },
      "source": [
        "## Interactive Sentiment Analysis\n",
        "\n",
        "### Subtask:\n",
        "Allow the user to input a sentence and get a real-time sentiment prediction (Positive/Negative) using the updated `predict_sentiment` function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "WhmBjphbuW6R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"omw-1.4\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"NLP pipeline: lowercase, remove non-letters, tokenize, remove stop-words, lemmatize.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"[^a-z\\\\s]\", \" \", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in STOP and len(t) > 1]\n",
        "    tokens = [LEMMATIZER.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"text_clean\"] = df[\"text\"].astype(str).apply(preprocess)\n",
        "print(\"Example:\")\n",
        "print(\"  Original:\", df[\"text\"].iloc[0][:80])\n",
        "print(\"  Cleaned: \", df[\"text_clean\"].iloc[0][:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6bJtBoruW29",
        "outputId": "49a634ad-5107-418f-fb36-e08ab2cd4dd8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example:\n",
            "  Original: Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. E\n",
            "  Cleaned:  dumb dumb thoroughly uninteresting supposed black comedy essentially start chris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text_clean\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=10_000, ngram_range=(1, 2), min_df=2)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_vec)\n",
        "print(\"Test accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBuZHOjLuW0n",
        "outputId": "64be26ea-a1b8-4ebb-e6af-05adc57ac056"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.852\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.87      0.83      0.85       503\n",
            "    Positive       0.83      0.88      0.85       497\n",
            "\n",
            "    accuracy                           0.85      1000\n",
            "   macro avg       0.85      0.85      0.85      1000\n",
            "weighted avg       0.85      0.85      0.85      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(sentence):\n",
        "    \"\"\"Given a sentence, return Positive or Negative using our NLP pipeline.\"\"\"\n",
        "    cleaned = preprocess(sentence)\n",
        "    X = vectorizer.transform([cleaned])\n",
        "    pred = model.predict(X)[0]\n",
        "    return \"Positive\" if pred == 1 else \"Negative\"\n",
        "\n",
        "# Try it on a few examples\n",
        "examples = [\n",
        "    \"I really enjoyed the movie!\",\n",
        "    \"This is the worst product ever.\",\n",
        "    \"Not bad, actually quite good.\",\n",
        "]\n",
        "for s in examples:\n",
        "    print(f\"  '{s}' -> {predict_sentiment(s)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5eFpvXVuWyO",
        "outputId": "063073c7-ebf8-44e7-c31f-4eddb6806e8a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  'I really enjoyed the movie!' -> Positive\n",
            "  'This is the worst product ever.' -> Negative\n",
            "  'Not bad, actually quite good.' -> Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this sentence and run the cell to get the predicted sentiment\n",
        "sentence = \"I love this place, the food was amazing and the staff were friendly.\"\n",
        "\n",
        "result = predict_sentiment(sentence)\n",
        "print(f\"Input: {sentence}\")\n",
        "print(f\"Sentiment: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHikq63CuWv7",
        "outputId": "d8e6f2a2-cab6-43a4-b61b-4d5278b09594"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I love this place, the food was amazing and the staff were friendly.\n",
            "Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87e11365"
      },
      "source": [
        "### PHASE 2 : Install Hugging Face Transformers and PyTorch\n",
        "\n",
        "### Subtask:\n",
        "Install the `transformers` library from Hugging Face and `torch`, which are necessary to work with transformer models. This step ensures all required dependencies for the new model are in place.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d76cf3de"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing the `transformers` library, so I will use a `code_block` to execute the pip install command for `transformers`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3d0c923",
        "outputId": "0e219c45-629f-40a7-b19f-ad42008e9cdc"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers\n",
        "print(\"Installed 'transformers' library.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Installed 'transformers' library.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cec6447d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask also requires installing the `torch` library, so I will use a `code_block` to execute the pip install command for `torch`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "122c5b84",
        "outputId": "65304e15-da73-434e-f191-95cffced5c80"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install torch\n",
        "print(\"Installed 'torch' library.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Installed 'torch' library.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f09dea32"
      },
      "source": [
        "**Reasoning**:\n",
        "To initialize the Hugging Face sentiment analysis pipeline, I need to import the `pipeline` function from the `transformers` library and then call it with 'sentiment-analysis' as the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5993111",
        "outputId": "791c755b-3365-4901-e350-76443d45fae5"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the sentiment analysis pipeline. This will download a pre-trained model.\n",
        "hf_sentiment_pipeline = pipeline('sentiment-analysis')\n",
        "\n",
        "print(\"Hugging Face sentiment analysis pipeline initialized successfully.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face sentiment analysis pipeline initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d402378d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully initialized the Hugging Face sentiment analysis pipeline. The next logical step, as per the main task, is to evaluate its performance on the `X_test` and `y_test` data. This involves making predictions with the new pipeline and comparing them to the true labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8262ee8d",
        "outputId": "aba54e17-8721-4d56-dc1e-5471c8c5a1b6"
      },
      "source": [
        "def predict_sentiment(sentence):\n",
        "    \"\"\"Given a sentence, return Positive or Negative using the Hugging Face sentiment analysis pipeline.\"\"\"\n",
        "    # Pass the sentence to the Hugging Face pipeline\n",
        "    result = hf_sentiment_pipeline(sentence)\n",
        "    # The pipeline returns a list of dictionaries, e.g., [{'label': 'POSITIVE', 'score': 0.999}]\n",
        "    label = result[0]['label']\n",
        "\n",
        "    # Map the Hugging Face labels to 'Positive' or 'Negative'\n",
        "    if label == 'POSITIVE':\n",
        "        return 'Positive'\n",
        "    elif label == 'NEGATIVE':\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        # In case of any unexpected label, though typically it's binary\n",
        "        return 'Neutral'\n",
        "\n",
        "print(\"The `predict_sentiment` function has been updated to use the Hugging Face model.\")\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The `predict_sentiment` function has been updated to use the Hugging Face model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7be01306"
      },
      "source": [
        "**Reasoning**:\n",
        "To confirm that the `predict_sentiment` function has been successfully updated and is using the Hugging Face model, I need to test it with example sentences and observe the output, similar to how the original function was tested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e6fdbf0",
        "outputId": "fd0fac5f-4b2d-4764-b049-d0f007d30e69"
      },
      "source": [
        "print('Testing the updated predict_sentiment function:')\n",
        "# Try it on a few examples\n",
        "examples = [\n",
        "    \"I really enjoyed the movie!\",\n",
        "    \"This is the worst product ever.\",\n",
        "    \"Not bad, actually quite good.\",\n",
        "    \"I hate this product, it's terrible and a waste of money.\",\n",
        "    \"The service was exceptional, I'm very happy.\"\n",
        "]\n",
        "for s in examples:\n",
        "    print(f\"  '{s}' -> {predict_sentiment(s)}\")\n",
        "\n",
        "# Also test with the previous `sentence` variable\n",
        "print(f\"\\nInput: {sentence}\")\n",
        "print(f\"Sentiment: {predict_sentiment(sentence)}\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the updated predict_sentiment function:\n",
            "  'I really enjoyed the movie!' -> Positive\n",
            "  'This is the worst product ever.' -> Negative\n",
            "  'Not bad, actually quite good.' -> Positive\n",
            "  'I hate this product, it's terrible and a waste of money.' -> Negative\n",
            "  'The service was exceptional, I'm very happy.' -> Positive\n",
            "\n",
            "Input: I love this place, the food was amazing and the staff were friendly.\n",
            "Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "334ca731"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the sentiment analysis has been successfully updated to use a Hugging Face transformer model, that the analysis is working, and that comments and descriptions have been added to the relevant code sections as requested.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6259977f"
      },
      "source": [
        "### Confirming the Hugging Face Model Used\n",
        "\n",
        "To explicitly see which model the `hf_sentiment_pipeline` is using, we can inspect its `model.name_or_path` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5f89d81",
        "outputId": "43be6328-fe49-4207-a9ca-c7adc27c7448"
      },
      "source": [
        "print(f\"The Hugging Face sentiment analysis pipeline is using the model: {hf_sentiment_pipeline.model.name_or_path}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Hugging Face sentiment analysis pipeline is using the model: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22d4b2c7",
        "outputId": "1d9871e3-09ed-4a0f-c22a-6fc8b1f30c5b"
      },
      "source": [
        "# Prompt the user for a sentence\n",
        "user_sentence = input(\"Enter a sentence for sentiment analysis: \")\n",
        "\n",
        "# Get the sentiment prediction using the updated function\n",
        "predicted_sentiment = predict_sentiment(user_sentence)\n",
        "\n",
        "# Print the input sentence and its predicted sentiment\n",
        "print(f\"\\nYour input: '{user_sentence}'\")\n",
        "print(f\"Predicted sentiment: {predicted_sentiment}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence for sentiment analysis: I am not well today. But I am happy that I get to spend time with my family.\n",
            "\n",
            "Your input: 'I am not well today. But I am happy that I get to spend time with my family.'\n",
            "Predicted sentiment: Positive\n"
          ]
        }
      ]
    }
  ]
}